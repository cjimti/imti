{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra: Practical Applications in ML\n",
    "\n",
    "This notebook generates visualizations for neural networks, attention mechanisms, and vectorized operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def forward_layer(x, W, b, activation='relu'):\n",
    "    \"\"\"Compute forward pass for one layer.\"\"\"\n",
    "    z = W @ x + b\n",
    "    if activation == 'relu':\n",
    "        return relu(z)\n",
    "    elif activation == 'sigmoid':\n",
    "        return sigmoid(z)\n",
    "    return z\n",
    "\n",
    "# Example: 3-layer network\n",
    "np.random.seed(42)\n",
    "\n",
    "# Layer dimensions: input=4, hidden1=8, hidden2=6, output=2\n",
    "W1 = np.random.randn(8, 4) * 0.5\n",
    "b1 = np.zeros(8)\n",
    "W2 = np.random.randn(6, 8) * 0.5\n",
    "b2 = np.zeros(6)\n",
    "W3 = np.random.randn(2, 6) * 0.5\n",
    "b3 = np.zeros(2)\n",
    "\n",
    "# Forward pass\n",
    "x = np.array([1.0, 2.0, 3.0, 4.0])\n",
    "h1 = forward_layer(x, W1, b1, 'relu')\n",
    "h2 = forward_layer(h1, W2, b2, 'relu')\n",
    "y = forward_layer(h2, W3, b3, 'sigmoid')\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Hidden layer 1 shape: {h1.shape}\")\n",
    "print(f\"Hidden layer 2 shape: {h2.shape}\")\n",
    "print(f\"Output shape: {y.shape}\")\n",
    "print(f\"Output: {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, 200)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# ReLU\n",
    "axes[0].plot(x, relu(x), 'b-', linewidth=2)\n",
    "axes[0].axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[0].axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('ReLU(x)')\n",
    "axes[0].set_title('ReLU: max(0, x)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Sigmoid\n",
    "axes[1].plot(x, sigmoid(x), 'r-', linewidth=2)\n",
    "axes[1].axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[1].axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[1].set_xlabel('x')\n",
    "axes[1].set_ylabel('sigmoid(x)')\n",
    "axes[1].set_title('Sigmoid: 1/(1+e^(-x))')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Softmax visualization (for 2 classes)\n",
    "z1 = np.linspace(-3, 3, 200)\n",
    "z2 = 0  # Fixed second logit\n",
    "softmax_probs = np.exp(z1) / (np.exp(z1) + np.exp(z2))\n",
    "axes[2].plot(z1, softmax_probs, 'g-', linewidth=2, label='P(class 1)')\n",
    "axes[2].plot(z1, 1 - softmax_probs, 'purple', linewidth=2, label='P(class 2)')\n",
    "axes[2].axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[2].set_xlabel('z1 (z2=0)')\n",
    "axes[2].set_ylabel('Probability')\n",
    "axes[2].set_title('Softmax (2 classes)')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../static/img/post/linear-algebra/activation-functions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Mechanism Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    \"\"\"Numerically stable softmax.\"\"\"\n",
    "    z_shifted = z - np.max(z, axis=-1, keepdims=True)\n",
    "    exp_z = np.exp(z_shifted)\n",
    "    return exp_z / np.sum(exp_z, axis=-1, keepdims=True)\n",
    "\n",
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    \"\"\"\n",
    "    Scaled dot-product attention.\n",
    "    Q, K, V: (seq_len, d_k)\n",
    "    \"\"\"\n",
    "    d_k = Q.shape[-1]\n",
    "\n",
    "    # Compute attention scores\n",
    "    scores = Q @ K.T / np.sqrt(d_k)\n",
    "\n",
    "    # Apply softmax\n",
    "    attention_weights = softmax(scores)\n",
    "\n",
    "    # Weighted sum of values\n",
    "    output = attention_weights @ V\n",
    "\n",
    "    return output, attention_weights\n",
    "\n",
    "# Example with meaningful attention pattern\n",
    "np.random.seed(123)\n",
    "seq_len = 6\n",
    "d_k = 8\n",
    "\n",
    "# Create Q, K, V with some structure\n",
    "Q = np.random.randn(seq_len, d_k)\n",
    "K = np.random.randn(seq_len, d_k)\n",
    "V = np.random.randn(seq_len, d_k)\n",
    "\n",
    "output, weights = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "# Visualize attention weights\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "im = ax.imshow(weights, cmap='Blues', aspect='auto')\n",
    "ax.set_xlabel('Key position')\n",
    "ax.set_ylabel('Query position')\n",
    "ax.set_title('Attention Weights Matrix')\n",
    "ax.set_xticks(range(seq_len))\n",
    "ax.set_yticks(range(seq_len))\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(im, ax=ax)\n",
    "cbar.set_label('Attention Weight')\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(seq_len):\n",
    "    for j in range(seq_len):\n",
    "        text = ax.text(j, i, f'{weights[i, j]:.2f}',\n",
    "                       ha='center', va='center', color='black' if weights[i, j] < 0.5 else 'white')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../static/img/post/linear-algebra/attention-weights.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(u, v):\n",
    "    \"\"\"Cosine similarity between two vectors.\"\"\"\n",
    "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))\n",
    "\n",
    "def cosine_similarity_matrix(X):\n",
    "    \"\"\"Pairwise cosine similarity matrix.\"\"\"\n",
    "    norms = np.linalg.norm(X, axis=1, keepdims=True)\n",
    "    X_normalized = X / norms\n",
    "    return X_normalized @ X_normalized.T\n",
    "\n",
    "# Example: word embeddings\n",
    "embeddings = {\n",
    "    'king': np.array([0.5, 0.3, 0.8, 0.1]),\n",
    "    'queen': np.array([0.5, 0.3, 0.7, 0.2]),\n",
    "    'man': np.array([0.2, 0.8, 0.3, 0.1]),\n",
    "    'woman': np.array([0.2, 0.8, 0.2, 0.2]),\n",
    "    'apple': np.array([0.9, 0.1, 0.1, 0.9]),\n",
    "}\n",
    "\n",
    "words = list(embeddings.keys())\n",
    "X = np.array([embeddings[w] for w in words])\n",
    "sim_matrix = cosine_similarity_matrix(X)\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "im = ax.imshow(sim_matrix, cmap='RdYlGn', vmin=0, vmax=1, aspect='auto')\n",
    "ax.set_xticks(range(len(words)))\n",
    "ax.set_yticks(range(len(words)))\n",
    "ax.set_xticklabels(words)\n",
    "ax.set_yticklabels(words)\n",
    "ax.set_title('Word Embedding Cosine Similarity')\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(im, ax=ax)\n",
    "cbar.set_label('Cosine Similarity')\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(len(words)):\n",
    "    for j in range(len(words)):\n",
    "        text = ax.text(j, i, f'{sim_matrix[i, j]:.2f}',\n",
    "                       ha='center', va='center', color='black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../static/img/post/linear-algebra/cosine-similarity.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(X, y, regularization=0):\n",
    "    \"\"\"Solve linear regression with optional L2 regularization.\"\"\"\n",
    "    X_bias = np.column_stack([np.ones(len(X)), X])\n",
    "    n_features = X_bias.shape[1]\n",
    "    XtX = X_bias.T @ X_bias\n",
    "    if regularization > 0:\n",
    "        XtX += regularization * np.eye(n_features)\n",
    "    Xty = X_bias.T @ y\n",
    "    weights = np.linalg.solve(XtX, Xty)\n",
    "    return weights\n",
    "\n",
    "# Generate 2D data for visualization\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(50, 1) * 2\n",
    "true_w = np.array([2.0, 1.5])  # [bias, slope]\n",
    "y = true_w[0] + true_w[1] * X.flatten() + np.random.randn(50) * 0.8\n",
    "\n",
    "# Fit model\n",
    "weights = linear_regression(X, y)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Regression line\n",
    "ax = axes[0]\n",
    "ax.scatter(X, y, alpha=0.6, s=50, label='Data')\n",
    "x_line = np.linspace(X.min(), X.max(), 100)\n",
    "y_line = weights[0] + weights[1] * x_line\n",
    "ax.plot(x_line, y_line, 'r-', linewidth=2, label=f'Fit: y = {weights[0]:.2f} + {weights[1]:.2f}x')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_title('Linear Regression via Normal Equations')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Effect of regularization\n",
    "ax = axes[1]\n",
    "regularizations = [0, 0.1, 1, 10]\n",
    "colors = plt.cm.viridis(np.linspace(0, 0.8, len(regularizations)))\n",
    "\n",
    "ax.scatter(X, y, alpha=0.3, s=30, color='gray')\n",
    "\n",
    "for reg, color in zip(regularizations, colors):\n",
    "    w = linear_regression(X, y, regularization=reg)\n",
    "    y_pred = w[0] + w[1] * x_line\n",
    "    ax.plot(x_line, y_pred, color=color, linewidth=2, label=f'lambda={reg}')\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_title('Effect of L2 Regularization')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../static/img/post/linear-algebra/linear-regression-regularization.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Architecture Diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_neural_net(ax, layer_sizes):\n",
    "    \"\"\"Draw a neural network diagram.\"\"\"\n",
    "    n_layers = len(layer_sizes)\n",
    "    max_neurons = max(layer_sizes)\n",
    "    \n",
    "    layer_positions = np.linspace(0, 1, n_layers)\n",
    "    \n",
    "    for i, (n_neurons, x) in enumerate(zip(layer_sizes, layer_positions)):\n",
    "        y_positions = np.linspace(0.1, 0.9, n_neurons)\n",
    "        \n",
    "        # Draw neurons\n",
    "        for y in y_positions:\n",
    "            circle = plt.Circle((x, y), 0.03, color='steelblue', ec='black', linewidth=1.5)\n",
    "            ax.add_patch(circle)\n",
    "        \n",
    "        # Draw connections to next layer\n",
    "        if i < n_layers - 1:\n",
    "            next_n = layer_sizes[i + 1]\n",
    "            next_x = layer_positions[i + 1]\n",
    "            next_y = np.linspace(0.1, 0.9, next_n)\n",
    "            \n",
    "            for y1 in y_positions:\n",
    "                for y2 in next_y:\n",
    "                    ax.plot([x + 0.03, next_x - 0.03], [y1, y2], 'gray', alpha=0.3, linewidth=0.5)\n",
    "\n",
    "    # Labels\n",
    "    labels = ['Input', 'Hidden 1', 'Hidden 2', 'Output']\n",
    "    for i, (x, label) in enumerate(zip(layer_positions, labels[:n_layers])):\n",
    "        ax.text(x, -0.05, label, ha='center', fontsize=10)\n",
    "        ax.text(x, 0.95, f'({layer_sizes[i]})', ha='center', fontsize=9, color='gray')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "draw_neural_net(ax, [4, 8, 6, 2])\n",
    "\n",
    "ax.set_xlim(-0.1, 1.1)\n",
    "ax.set_ylim(-0.15, 1.05)\n",
    "ax.set_aspect('equal')\n",
    "ax.axis('off')\n",
    "ax.set_title('Neural Network: Matrix Multiplication Chain', fontsize=12, pad=20)\n",
    "\n",
    "# Add equations\n",
    "ax.text(0.5, -0.12, r'$y = \\sigma(W_3 \\cdot \\text{ReLU}(W_2 \\cdot \\text{ReLU}(W_1 \\cdot x + b_1) + b_2) + b_3)$',\n",
    "        ha='center', fontsize=11, transform=ax.transAxes)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../static/img/post/linear-algebra/neural-network-diagram.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
