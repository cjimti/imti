<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI on IMTI</title>
    <link>https://imti.co/tags/ai/</link>
    <description>Recent content in AI on IMTI</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 20 Jan 2026 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://imti.co/tags/ai/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>MCP Is Flawed. Build With It Anyway.</title>
      <link>https://imti.co/mcp-defense/</link>
      <pubDate>Tue, 20 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://imti.co/mcp-defense/</guid>
      <description>&lt;h2 id=&#34;tldr&#34;&gt;TL;DR&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;MCP has real flaws&lt;/strong&gt;: security CVEs, scalability limits, immature OAuth. The critics aren&amp;rsquo;t wrong.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Most criticism targets implementations and deployment mistakes&lt;/strong&gt;, not the protocol itself. SQL injection didn&amp;rsquo;t kill SQL; we learned parameterized queries.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;For internal data platforms, the threat model changes entirely&lt;/strong&gt;. You control the server, the network, the access model.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Context and metadata have always been the hard problem in data&lt;/strong&gt;. MCP just makes the cost of bad metadata immediately visible.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Building MCP servers forces metadata investment that pays off regardless&lt;/strong&gt;. If MCP dies tomorrow, you still have documented, well-governed data.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;I&amp;rsquo;ve been experimenting with MCP for about six months now, building custom servers for data platform clients. Recently I started converting POCs and various experiments into legitimate OSS projects: &lt;a href=&#34;../../mcp-trino/&#34;&gt;mcp-trino&lt;/a&gt; for data warehouse access, &lt;a href=&#34;../../mcp-s3/&#34;&gt;mcp-s3&lt;/a&gt; for object storage, &lt;a href=&#34;https://github.com/txn2/mcp-datahub&#34;&gt;mcp-datahub&lt;/a&gt; for semantic context. The three work together. mcp-datahub provides the metadata layer that makes mcp-trino and mcp-s3 useful. Not because the world needs another generic MCP server. I build them as composable Go libraries, and public repositories put me in a different mindset. With OSS I put more effort into things like keeping documentation up to date.&lt;/p&gt;&#xA;&lt;p&gt;MCP has problems. Serious ones. The security model is immature. The context window math doesn&amp;rsquo;t work at scale. The OAuth spec is a mess. I&amp;rsquo;ve read the criticism, and most of it is technically accurate.&lt;/p&gt;&#xA;&lt;p&gt;I&amp;rsquo;m building with MCP anyway. Here&amp;rsquo;s why.&lt;/p&gt;</description>
    </item>
    <item>
      <title>AI Data Lake Access with MCP and S3</title>
      <link>https://imti.co/mcp-s3/</link>
      <pubDate>Mon, 19 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://imti.co/mcp-s3/</guid>
      <description>&lt;p&gt;Data warehouses handle structured queries. Data lakes handle everything else: documents, images, logs, backups, ML model weights, that CSV someone exported three years ago and never deleted. The S3 API has become the universal interface for object storage. AWS built it, and now MinIO, SeaweedFS, Ceph, and dozens of others implement it. &lt;a href=&#34;https://github.com/txn2/mcp-s3&#34;&gt;txn2/mcp-s3&lt;/a&gt; brings this storage layer to AI assistants through MCP, working alongside &lt;a href=&#34;../../mcp-trino/&#34;&gt;txn2/mcp-trino&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>AI Data Warehouse Access with MCP and Trino</title>
      <link>https://imti.co/mcp-trino/</link>
      <pubDate>Sat, 17 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://imti.co/mcp-trino/</guid>
      <description>&lt;p&gt;Every organization wants AI assistants to answer questions from their data. The problem is that data lives everywhere: PostgreSQL for transactions, MySQL for legacy systems, S3 for analytics, Kafka for streams. Writing custom integrations for each source is unsustainable. &lt;a href=&#34;https://github.com/txn2/mcp-trino&#34;&gt;mcp-trino&lt;/a&gt; solves this by exposing Trino&amp;rsquo;s federated query engine to AI assistants through MCP.&lt;/p&gt;</description>
    </item>
    <item>
      <title>kubefwd in 2026: Interactive TUI and Auto-Reconnect</title>
      <link>https://imti.co/kubefwd-2026/</link>
      <pubDate>Sun, 04 Jan 2026 12:01:00 +0000</pubDate>
      <guid>https://imti.co/kubefwd-2026/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://github.com/txn2/kubefwd&#34;&gt;kubefwd&lt;/a&gt; has been a stable and useful tool for my team and me for almost eight years. It solves a simple but persistent problem: developing applications locally that need to communicate with services running in Kubernetes. Rather than juggling multiple &lt;code&gt;kubectl port-forward&lt;/code&gt; commands or maintaining environment-specific connection strings, kubefwd bulk-forwards services so they&amp;rsquo;re accessible by name, just like they would be inside the cluster.&lt;/p&gt;&#xA;&lt;p&gt;After years of reliable service (aside from Windows &lt;code&gt;/etc/hosts&lt;/code&gt; limitations and reconnection logic that are now resolved), I finally added features that people had been requesting: auto-reconnect, an interactive terminal UI, and a REST API for programmatic control. This article covers what&amp;rsquo;s new.&lt;/p&gt;</description>
    </item>
    <item>
      <title>AI-Assisted Kubernetes Development with kubefwd</title>
      <link>https://imti.co/kubefwd-mcp/</link>
      <pubDate>Sun, 04 Jan 2026 12:00:00 +0000</pubDate>
      <guid>https://imti.co/kubefwd-mcp/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://github.com/txn2/kubefwd&#34;&gt;kubefwd&lt;/a&gt; is a tool that bulk-forwards Kubernetes services to your local machine, making them accessible by their real hostnames. Instead of juggling &lt;code&gt;kubectl port-forward&lt;/code&gt; commands, you run &lt;code&gt;sudo kubefwd svc -n mynamespace&lt;/code&gt; and your app can connect to &lt;code&gt;postgres:5432&lt;/code&gt; or &lt;code&gt;api:8080&lt;/code&gt; as if those services were running locally. For full details on kubefwd&amp;rsquo;s features, see &lt;a href=&#34;../../kubefwd-2026/&#34;&gt;kubefwd in 2026: Interactive TUI and Auto-Reconnect&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;This article focuses on kubefwd&amp;rsquo;s MCP integration, which lets AI assistants manage port forwarding on your behalf.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
